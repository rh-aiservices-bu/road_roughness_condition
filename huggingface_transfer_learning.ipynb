{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3260669-c7ec-4d06-a655-590c5e7ab152",
   "metadata": {},
   "source": [
    "# Transfer learning with Huggingface using CodeFlare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acfb10-1aa1-445d-947e-396ea5ebed1a",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to leverage the **[huggingface](https://huggingface.co/)** support in ray ecosystem to carry out a text classification task using transfer learning. We will be referencing the example **[here](https://huggingface.co/docs/transformers/tasks/sequence_classification)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b77929-e96c-434e-ada3-8b14795bfbb1",
   "metadata": {},
   "source": [
    "The example carries out a text classification task on **[imdb dataset](https://huggingface.co/datasets/imdb)** and tries to classify the movie reviews as positive or negative. Huggingface library provides an easy way to build a model and the dataset to carry out this classification task. In this case we will be using **distilbert-base-uncased** model which is a **BERT** based model.\n",
    "\n",
    "Huggingface has a **[built in support for ray ecosystem](https://docs.ray.io/en/releases-1.13.0/_modules/ray/ml/train/integrations/huggingface/huggingface_trainer.html)** which allows the huggingface trainer to scale on CodeFlare and can scale the training as we add additional gpus and can run distributed training across multiple GPUs that will help scale out the training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02593d04-40b9-4a07-a32e-40b649444ab5",
   "metadata": {},
   "source": [
    "### Getting all the requirements in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f762ff1-892e-4eeb-b7ad-3ae1a242dc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into \"https://your-cluster\" as \"kube:admin\" using the token provided.\n",
      "\n",
      "You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"rhods-notebooks\".\n"
     ]
    }
   ],
   "source": [
    "! oc login --token=your-token --server=https://your-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1cdc80-8c5c-440e-a917-f561f6b2c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using project \"default\" on server \"https://your-cluster3\".\n"
     ]
    }
   ],
   "source": [
    "! oc project default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e05ec3-b09a-4792-acbf-1da7f6562ad0",
   "metadata": {},
   "source": [
    "Let's check that we have the necessary Hugging Face packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76d73f8-fcc7-4012-b813-1b4c12002d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1aa5960-7bd7-4e61-a903-19002940f999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.23.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d8c661-6056-4906-b5c4-9504790ca043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "evaluate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c737a768-6e31-4767-a301-60ae932b4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220b9d85-3a3c-4c0c-aaf2-0d866823dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to: hfgputest.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create our cluster and submit appwrapper\n",
    "cluster = Cluster(ClusterConfiguration(name='hfgputest', min_worker=1, max_worker=1, min_cpus=8, max_cpus=8, min_memory=16, max_memory=16, gpu=4, instascale=True, machine_types=[\"m5.xlarge\", \"p3.8xlarge\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae1d861-b743-4c05-903b-5799072b942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0db5f5-22f1-4806-ae7e-a0ee865625c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ <span style=\"font-style: italic\"> </span><span style=\"font-weight: bold; font-style: italic\"> ðŸš€ List of CodeFlare</span><span style=\"font-style: italic\"> </span> â”‚\n",
       "â”‚ <span style=\"font-style: italic\">  </span><span style=\"font-weight: bold; font-style: italic\">clusters in queueðŸš€</span><span style=\"font-style: italic\">  </span> â”‚\n",
       "â”‚ +-----------+---------+ â”‚\n",
       "â”‚ |<span style=\"font-weight: bold\"> Name      </span>|<span style=\"font-weight: bold\"> Status  </span>| â”‚\n",
       "â”‚ +===========+=========+ â”‚\n",
       "â”‚ |<span style=\"color: #008080; text-decoration-color: #008080\"> hfgputest </span>|<span style=\"color: #800080; text-decoration-color: #800080\"> pending </span>| â”‚\n",
       "â”‚ |<span style=\"color: #008080; text-decoration-color: #008080\">           </span>|<span style=\"color: #800080; text-decoration-color: #800080\">         </span>| â”‚\n",
       "â”‚ +-----------+---------+ â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ \u001b[3m \u001b[0m\u001b[1;3m ðŸš€ List of CodeFlare\u001b[0m\u001b[3m \u001b[0m â”‚\n",
       "â”‚ \u001b[3m  \u001b[0m\u001b[1;3mclusters in queueðŸš€\u001b[0m\u001b[3m  \u001b[0m â”‚\n",
       "â”‚ +-----------+---------+ â”‚\n",
       "â”‚ |\u001b[1m \u001b[0m\u001b[1mName     \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mStatus \u001b[0m\u001b[1m \u001b[0m| â”‚\n",
       "â”‚ +===========+=========+ â”‚\n",
       "â”‚ |\u001b[36m \u001b[0m\u001b[36mhfgputest\u001b[0m\u001b[36m \u001b[0m|\u001b[35m \u001b[0m\u001b[35mpending\u001b[0m\u001b[35m \u001b[0m| â”‚\n",
       "â”‚ |\u001b[36m \u001b[0m\u001b[36m         \u001b[0m\u001b[36m \u001b[0m|\u001b[35m \u001b[0m\u001b[35m       \u001b[0m\u001b[35m \u001b[0m| â”‚\n",
       "â”‚ +-----------+---------+ â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(False, <CodeFlareClusterStatus.QUEUED: 2>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a54428-f186-4c27-948e-4eaf9c0e34b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                 </span><span style=\"font-weight: bold; font-style: italic\"> ðŸš€ List of CodeFlare clusters ðŸš€</span><span style=\"font-style: italic\">                  </span>\n",
       "<span style=\"font-weight: bold\">                                                                    </span>\n",
       " â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® \n",
       " â”‚   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #008000; font-weight: bold\">Owner</span>                                                        â”‚ \n",
       " â”‚   <span style=\"font-weight: bold; text-decoration: underline\">hfgputest</span>                                        Active âœ…   â”‚ \n",
       " â”‚                                                                â”‚ \n",
       " â”‚   <span style=\"font-weight: bold\">URI:</span> ray://hfgputest-head-svc.default.svc:10001              â”‚ \n",
       " â”‚                                                                â”‚ \n",
       " â”‚   <a href=\"ray-dashboard-hfgputest-default.apps.prepfullinstall.psap.aws.rhperfscale.org\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">DashboardðŸ”—</span></a>                                                  â”‚ \n",
       " â”‚                                                                â”‚ \n",
       " â”‚  <span style=\"font-style: italic\">                    Cluster Resources                     </span>    â”‚ \n",
       " â”‚   â•­â”€ Workers â”€â”€â•®  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker specs(each) â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®     â”‚ \n",
       " â”‚   â”‚ <span style=\"font-weight: bold\"> Min  Max </span> â”‚  â”‚ <span style=\"font-weight: bold\"> Memory      CPU         GPU        </span> â”‚     â”‚ \n",
       " â”‚   â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">     </span><span style=\"color: #800080; text-decoration-color: #800080\">     </span> â”‚  â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">            </span><span style=\"color: #800080; text-decoration-color: #800080\">                        </span> â”‚     â”‚ \n",
       " â”‚   â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 1   </span><span style=\"color: #800080; text-decoration-color: #800080\"> 1   </span> â”‚  â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 16G~16G    </span><span style=\"color: #800080; text-decoration-color: #800080\"> 8           4          </span> â”‚     â”‚ \n",
       " â”‚   â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">     </span><span style=\"color: #800080; text-decoration-color: #800080\">     </span> â”‚  â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">            </span><span style=\"color: #800080; text-decoration-color: #800080\">                        </span> â”‚     â”‚ \n",
       " â”‚   â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â”‚ \n",
       " â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                 \u001b[0m\u001b[1;3m ðŸš€ List of CodeFlare clusters ðŸš€\u001b[0m\u001b[3m                  \u001b[0m\n",
       "\u001b[1m \u001b[0m\u001b[1m                                                                  \u001b[0m\u001b[1m \u001b[0m\n",
       " â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® \n",
       " â”‚   \u001b[1;37;42mOwner\u001b[0m                                                        â”‚ \n",
       " â”‚   \u001b[1;4mhfgputest\u001b[0m                                        Active âœ…   â”‚ \n",
       " â”‚                                                                â”‚ \n",
       " â”‚   \u001b[1mURI:\u001b[0m ray://hfgputest-head-svc.default.svc:10001              â”‚ \n",
       " â”‚                                                                â”‚ \n",
       " â”‚   \u001b]8;id=572476;ray-dashboard-hfgputest-default.apps.prepfullinstall.psap.aws.rhperfscale.org\u001b\\\u001b[4;34mDashboardðŸ”—\u001b[0m\u001b]8;;\u001b\\                                                  â”‚ \n",
       " â”‚                                                                â”‚ \n",
       " â”‚  \u001b[3m                    Cluster Resources                     \u001b[0m    â”‚ \n",
       " â”‚   â•­â”€ Workers â”€â”€â•®  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker specs(each) â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®     â”‚ \n",
       " â”‚   â”‚ \u001b[1m \u001b[0m\u001b[1mMin\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mMax\u001b[0m\u001b[1m \u001b[0m â”‚  â”‚ \u001b[1m \u001b[0m\u001b[1mMemory    \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCPU       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mGPU       \u001b[0m\u001b[1m \u001b[0m â”‚     â”‚ \n",
       " â”‚   â”‚ \u001b[36m \u001b[0m\u001b[36m   \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   \u001b[0m\u001b[35m \u001b[0m â”‚  â”‚ \u001b[36m \u001b[0m\u001b[36m          \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m â”‚     â”‚ \n",
       " â”‚   â”‚ \u001b[36m \u001b[0m\u001b[36m1  \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m1  \u001b[0m\u001b[35m \u001b[0m â”‚  â”‚ \u001b[36m \u001b[0m\u001b[36m16G~16G   \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m8         \u001b[0m\u001b[35m \u001b[0m\u001b[35m \u001b[0m\u001b[35m4         \u001b[0m\u001b[35m \u001b[0m â”‚     â”‚ \n",
       " â”‚   â”‚ \u001b[36m \u001b[0m\u001b[36m   \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   \u001b[0m\u001b[35m \u001b[0m â”‚  â”‚ \u001b[36m \u001b[0m\u001b[36m          \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m\u001b[35m \u001b[0m\u001b[35m          \u001b[0m\u001b[35m \u001b[0m â”‚     â”‚ \n",
       " â”‚   â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â”‚ \n",
       " â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<RayClusterStatus.READY: 'ready'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac46c87-70f1-4c70-9648-881151665355",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_cluster_uri = cluster.cluster_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dba6a0-8275-4726-8911-6b6ec467b6a3",
   "metadata": {},
   "source": [
    "**NOTE**: Here we have created a custom cluster with a GPU. You can add more GPUs by changing the spec above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c458589-5a17-47c6-a8db-625427ae4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster is up and running:  True\n"
     ]
    }
   ],
   "source": [
    "#before proceeding make sure the cluster exists and the uri is not empty\n",
    "assert ray_cluster_uri, \"Ray cluster needs to be started and set before proceeding\"\n",
    "\n",
    "import ray\n",
    "\n",
    "# reset the ray context in case there's already one. \n",
    "ray.shutdown()\n",
    "# establish connection to ray cluster\n",
    "\n",
    "#install additionall libraries that will be required for this training\n",
    "runtime_env = {\"pip\": [\"transformers\", \"datasets\", \"evaluate\"]}\n",
    "\n",
    "ray.init(address=f'{ray_cluster_uri}', runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a38146-1321-4b7b-9152-9ebca4eb9444",
   "metadata": {},
   "source": [
    "**NOTE** : in this case since we are running a task for which we need additional pip packages. we can install those by passing them in the `runtime_env` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1945b-d6c8-49b8-9a4c-b82724cffba9",
   "metadata": {},
   "source": [
    "### Transfer learning code from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbe888-4f38-4e9a-ae43-67ce89ff9d42",
   "metadata": {},
   "source": [
    "We are using the code based on the example **[here](https://huggingface.co/docs/transformers/tasks/sequence_classification)** . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e69994b4-1a13-43fe-b698-2a5374cb941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_fn():\n",
    "    from datasets import load_dataset\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, TrainingArguments\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    import numpy as np\n",
    "    from datasets import load_metric\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.ml.train.integrations.huggingface import HuggingFaceTrainer\n",
    "\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    #using a fraction of dataset but you can run with the full dataset\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "    print(f\"len of train {small_train_dataset} and test {small_eval_dataset}\")\n",
    "\n",
    "    ray_train_ds = ray.data.from_huggingface(small_train_dataset)\n",
    "    ray_evaluation_ds = ray.data.from_huggingface(small_eval_dataset)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    def trainer_init_per_worker(train_dataset, eval_dataset, **config):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "        training_args = TrainingArguments(\"/tmp/hf_imdb/test\", eval_steps=1, disable_tqdm=True, \n",
    "                                          num_train_epochs=1, skip_memory_metrics=True,\n",
    "                                          learning_rate=2e-5,\n",
    "                                          per_device_train_batch_size=16,\n",
    "                                          per_device_eval_batch_size=16,                                \n",
    "                                          weight_decay=0.01,)\n",
    "        return transformers.Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "    scaling_config = {\"num_workers\": 4, \"use_gpu\": True} #num workers is the number of gpus\n",
    "\n",
    "    # we are using the ray native HuggingFaceTrainer, but you can swap out to use non ray Huggingface Trainer. Both have the same method signature. \n",
    "    # the ray native HFTrainer has built in support for scaling to multiple GPUs\n",
    "    trainer = HuggingFaceTrainer(\n",
    "        trainer_init_per_worker=trainer_init_per_worker,\n",
    "        scaling_config=scaling_config,\n",
    "        datasets={\"train\": ray_train_ds, \"evaluation\": ray_evaluation_ds},\n",
    "    )\n",
    "    result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9593fee-2b2b-415f-8902-bceec014385f",
   "metadata": {},
   "source": [
    "**NOTE:** This code will produce a lot of output and will run for **approximately 2 minutes.** As a part of execution it will download the `imdb` dataset, `distilbert-base-uncased` model and then will start transfer learning task for training the model with this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f0985e9-5e88-4d36-ab38-c3001c13f97c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.31k/4.31k [00:00<00:00, 5.26MB/s]\n",
      "Downloading metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.17k/2.17k [00:00<00:00, 3.10MB/s]\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.59k/7.59k [00:00<00:00, 9.32MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Downloading and preparing dataset imdb/plain_text to /home/ray/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]\n",
      "Downloading data:   0%|          | 32.8k/84.1M [00:00<05:06, 274kB/s]\n",
      "Downloading data:   0%|          | 97.3k/84.1M [00:00<03:16, 429kB/s]\n",
      "Downloading data:   0%|          | 194k/84.1M [00:00<02:20, 598kB/s] \n",
      "Downloading data:   0%|          | 367k/84.1M [00:00<01:30, 931kB/s]\n",
      "Downloading data:   1%|          | 645k/84.1M [00:00<00:58, 1.43MB/s]\n",
      "Downloading data:   1%|â–         | 1.06M/84.1M [00:00<00:39, 2.10MB/s]\n",
      "Downloading data:   2%|â–         | 1.60M/84.1M [00:00<00:28, 2.90MB/s]\n",
      "Downloading data:   3%|â–Ž         | 2.27M/84.1M [00:00<00:21, 3.74MB/s]\n",
      "Downloading data:   4%|â–Ž         | 3.08M/84.1M [00:01<00:17, 4.69MB/s]\n",
      "Downloading data:   5%|â–         | 4.17M/84.1M [00:01<00:13, 6.03MB/s]\n",
      "Downloading data:   7%|â–‹         | 5.65M/84.1M [00:01<00:09, 7.95MB/s]\n",
      "Downloading data:  10%|â–‰         | 8.10M/84.1M [00:01<00:06, 12.1MB/s]\n",
      "Downloading data:  14%|â–ˆâ–        | 11.7M/84.1M [00:01<00:03, 18.5MB/s]\n",
      "Downloading data:  17%|â–ˆâ–‹        | 14.3M/84.1M [00:01<00:03, 20.8MB/s]\n",
      "Downloading data:  21%|â–ˆâ–ˆ        | 17.8M/84.1M [00:01<00:02, 24.8MB/s]\n",
      "Downloading data:  27%|â–ˆâ–ˆâ–‹       | 22.5M/84.1M [00:01<00:01, 31.1MB/s]\n",
      "Downloading data:  31%|â–ˆâ–ˆâ–ˆ       | 25.8M/84.1M [00:01<00:01, 31.6MB/s]\n",
      "Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–      | 29.0M/84.1M [00:02<00:01, 31.7MB/s]\n",
      "Downloading data:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 33.6M/84.1M [00:02<00:01, 34.0MB/s]\n",
      "Downloading data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 38.4M/84.1M [00:02<00:01, 35.8MB/s]\n",
      "Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 43.2M/84.1M [00:02<00:01, 37.1MB/s]\n",
      "Downloading data:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 48.0M/84.1M [00:02<00:00, 40.2MB/s]\n",
      "Downloading data:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 52.1M/84.1M [00:02<00:00, 40.0MB/s]\n",
      "Downloading data:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 56.1M/84.1M [00:02<00:00, 40.0MB/s]\n",
      "Downloading data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 61.1M/84.1M [00:02<00:00, 42.8MB/s]\n",
      "Downloading data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 65.4M/84.1M [00:02<00:00, 42.5MB/s]\n",
      "Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 69.6M/84.1M [00:03<00:00, 41.7MB/s]\n",
      "Downloading data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 74.4M/84.1M [00:03<00:00, 42.1MB/s]\n",
      "Downloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 79.4M/84.1M [00:03<00:00, 44.2MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84.1M/84.1M [00:03<00:00, 25.3MB/s]\n",
      "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
      "Generating train split:   0%|          | 1/25000 [00:02<15:18:03,  2.20s/ examples]\n",
      "Generating train split:   3%|â–Ž         | 700/25000 [00:02<00:57, 425.99 examples/s]\n",
      "Generating train split:   6%|â–Œ         | 1461/25000 [00:02<00:23, 1000.80 examples/s]\n",
      "Generating train split:   9%|â–‰         | 2235/25000 [00:02<00:13, 1692.08 examples/s]\n",
      "Generating train split:  12%|â–ˆâ–        | 3003/25000 [00:02<00:08, 2457.34 examples/s]\n",
      "Generating train split:  15%|â–ˆâ–Œ        | 3766/25000 [00:02<00:06, 3255.84 examples/s]\n",
      "Generating train split:  18%|â–ˆâ–Š        | 4483/25000 [00:02<00:05, 3960.93 examples/s]\n",
      "Generating train split:  21%|â–ˆâ–ˆ        | 5252/25000 [00:02<00:04, 4724.64 examples/s]\n",
      "Generating train split:  24%|â–ˆâ–ˆâ–       | 6026/25000 [00:03<00:03, 5407.58 examples/s]\n",
      "Generating train split:  27%|â–ˆâ–ˆâ–‹       | 6772/25000 [00:03<00:03, 5817.03 examples/s]\n",
      "Generating train split:  30%|â–ˆâ–ˆâ–ˆ       | 7549/25000 [00:03<00:02, 6314.90 examples/s]\n",
      "Generating train split:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8321/25000 [00:03<00:02, 6689.43 examples/s]\n",
      "Generating train split:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 9093/25000 [00:03<00:02, 6973.64 examples/s]\n",
      "Generating train split:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9870/25000 [00:03<00:02, 7197.96 examples/s]\n",
      "Generating train split:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10635/25000 [00:03<00:02, 6587.42 examples/s]\n",
      "Generating train split:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11416/25000 [00:03<00:01, 6916.11 examples/s]\n",
      "Generating train split:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 12198/25000 [00:03<00:01, 7167.40 examples/s]\n",
      "Generating train split:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12988/25000 [00:03<00:01, 7374.02 examples/s]\n",
      "Generating train split:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13758/25000 [00:04<00:01, 7468.06 examples/s]\n",
      "Generating train split:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 14538/25000 [00:04<00:01, 7562.99 examples/s]\n",
      "Generating train split:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15319/25000 [00:04<00:01, 7634.76 examples/s]\n",
      "Generating train split:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16094/25000 [00:04<00:01, 7665.91 examples/s]\n",
      "Generating train split:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16869/25000 [00:04<00:01, 7687.62 examples/s]\n",
      "Generating train split:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 17644/25000 [00:04<00:00, 7704.56 examples/s]\n",
      "Generating train split:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 18418/25000 [00:04<00:00, 7714.02 examples/s]\n",
      "Generating train split:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 19195/25000 [00:04<00:00, 7730.49 examples/s]\n",
      "Generating train split:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19971/25000 [00:04<00:00, 7733.62 examples/s]\n",
      "Generating train split:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 20746/25000 [00:04<00:00, 7004.98 examples/s]\n",
      "Generating train split:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 21511/25000 [00:05<00:00, 7182.04 examples/s]\n",
      "Generating train split:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 22288/25000 [00:05<00:00, 7344.58 examples/s]\n",
      "Generating train split:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23031/25000 [00:05<00:00, 7335.63 examples/s]\n",
      "Generating train split:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 23795/25000 [00:05<00:00, 7422.76 examples/s]\n",
      "Generating train split:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 24564/25000 [00:05<00:00, 7498.80 examples/s]\n",
      "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]               \n",
      "Generating test split:   0%|          | 1/25000 [00:00<3:10:03,  2.19 examples/s]\n",
      "Generating test split:   3%|â–Ž         | 759/25000 [00:00<00:13, 1809.39 examples/s]\n",
      "Generating test split:   6%|â–Œ         | 1537/25000 [00:00<00:07, 3326.02 examples/s]\n",
      "Generating test split:   9%|â–‰         | 2318/25000 [00:00<00:05, 4519.58 examples/s]\n",
      "Generating test split:  12%|â–ˆâ–        | 3104/25000 [00:00<00:04, 5439.87 examples/s]\n",
      "Generating test split:  16%|â–ˆâ–Œ        | 3889/25000 [00:00<00:03, 6119.61 examples/s]\n",
      "Generating test split:  19%|â–ˆâ–Š        | 4673/25000 [00:01<00:03, 6612.33 examples/s]\n",
      "Generating test split:  22%|â–ˆâ–ˆâ–       | 5451/25000 [00:01<00:02, 6950.65 examples/s]\n",
      "Generating test split:  25%|â–ˆâ–ˆâ–       | 6231/25000 [00:01<00:02, 7196.42 examples/s]\n",
      "Generating test split:  28%|â–ˆâ–ˆâ–Š       | 7005/25000 [00:01<00:02, 7354.81 examples/s]\n",
      "Generating test split:  31%|â–ˆâ–ˆâ–ˆ       | 7774/25000 [00:01<00:02, 7426.23 examples/s]\n",
      "Generating test split:  34%|â–ˆâ–ˆâ–ˆâ–      | 8540/25000 [00:01<00:02, 7435.57 examples/s]\n",
      "Generating test split:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 9314/25000 [00:01<00:02, 7524.95 examples/s]\n",
      "Generating test split:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10078/25000 [00:01<00:02, 6846.37 examples/s]\n",
      "Generating test split:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10843/25000 [00:01<00:02, 7068.98 examples/s]\n",
      "Generating test split:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11619/25000 [00:01<00:01, 7263.02 examples/s]\n",
      "Generating test split:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 12400/25000 [00:02<00:01, 7419.46 examples/s]\n",
      "Generating test split:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 13171/25000 [00:02<00:01, 7501.94 examples/s]\n",
      "Generating test split:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13943/25000 [00:02<00:01, 7563.54 examples/s]\n",
      "Generating test split:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14720/25000 [00:02<00:01, 7621.97 examples/s]\n",
      "Generating test split:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15503/25000 [00:02<00:01, 7676.57 examples/s]\n",
      "Generating test split:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 16289/25000 [00:02<00:01, 7728.62 examples/s]\n",
      "Generating test split:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17064/25000 [00:02<00:01, 7714.23 examples/s]\n",
      "Generating test split:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17837/25000 [00:02<00:00, 7711.56 examples/s]\n",
      "Generating test split:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18615/25000 [00:02<00:00, 7730.15 examples/s]\n",
      "Generating test split:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 19392/25000 [00:02<00:00, 7739.99 examples/s]\n",
      "Generating test split:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20167/25000 [00:03<00:00, 7062.55 examples/s]\n",
      "Generating test split:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20939/25000 [00:03<00:00, 7244.19 examples/s]\n",
      "Generating test split:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 21707/25000 [00:03<00:00, 7368.02 examples/s]\n",
      "Generating test split:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 22484/25000 [00:03<00:00, 7482.88 examples/s]\n",
      "Generating test split:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 23238/25000 [00:03<00:00, 7368.80 examples/s]\n",
      "Generating test split:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24019/25000 [00:03<00:00, 7496.07 examples/s]\n",
      "Generating test split:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 24801/25000 [00:03<00:00, 7589.24 examples/s]\n",
      "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]      \n",
      "Generating unsupervised split:   0%|          | 1/50000 [00:04<56:52:29,  4.10s/ examples]\n",
      "Generating unsupervised split:   2%|â–         | 759/50000 [00:04<03:12, 255.75 examples/s]\n",
      "Generating unsupervised split:   3%|â–Ž         | 1535/50000 [00:04<01:20, 600.50 examples/s]\n",
      "Generating unsupervised split:   5%|â–         | 2306/50000 [00:04<00:45, 1038.36 examples/s]\n",
      "Generating unsupervised split:   6%|â–Œ         | 3082/50000 [00:04<00:29, 1581.78 examples/s]\n",
      "Generating unsupervised split:   8%|â–Š         | 3789/50000 [00:04<00:21, 2149.99 examples/s]\n",
      "Generating unsupervised split:   9%|â–‰         | 4564/50000 [00:04<00:15, 2871.17 examples/s]\n",
      "Generating unsupervised split:  11%|â–ˆ         | 5348/50000 [00:04<00:12, 3642.90 examples/s]\n",
      "Generating unsupervised split:  12%|â–ˆâ–        | 6094/50000 [00:04<00:11, 3959.90 examples/s]\n",
      "Generating unsupervised split:  14%|â–ˆâ–Ž        | 6864/50000 [00:05<00:09, 4676.20 examples/s]\n",
      "Generating unsupervised split:  15%|â–ˆâ–Œ        | 7642/50000 [00:05<00:07, 5342.27 examples/s]\n",
      "Generating unsupervised split:  17%|â–ˆâ–‹        | 8418/50000 [00:05<00:07, 5908.83 examples/s]\n",
      "Generating unsupervised split:  18%|â–ˆâ–Š        | 9188/50000 [00:05<00:06, 6356.56 examples/s]\n",
      "Generating unsupervised split:  20%|â–ˆâ–‰        | 9960/50000 [00:05<00:05, 6714.25 examples/s]\n",
      "Generating unsupervised split:  21%|â–ˆâ–ˆâ–       | 10715/50000 [00:05<00:06, 6320.34 examples/s]\n",
      "Generating unsupervised split:  23%|â–ˆâ–ˆâ–Ž       | 11483/50000 [00:05<00:05, 6677.01 examples/s]\n",
      "Generating unsupervised split:  25%|â–ˆâ–ˆâ–       | 12256/50000 [00:05<00:05, 6962.11 examples/s]\n",
      "Generating unsupervised split:  26%|â–ˆâ–ˆâ–Œ       | 13030/50000 [00:05<00:05, 7177.96 examples/s]\n",
      "Generating unsupervised split:  28%|â–ˆâ–ˆâ–Š       | 13811/50000 [00:05<00:04, 7353.88 examples/s]\n",
      "Generating unsupervised split:  29%|â–ˆâ–ˆâ–‰       | 14587/50000 [00:06<00:04, 7470.25 examples/s]\n",
      "Generating unsupervised split:  31%|â–ˆâ–ˆâ–ˆ       | 15362/50000 [00:06<00:04, 7550.73 examples/s]\n",
      "Generating unsupervised split:  32%|â–ˆâ–ˆâ–ˆâ–      | 16141/50000 [00:06<00:04, 7620.69 examples/s]\n",
      "Generating unsupervised split:  34%|â–ˆâ–ˆâ–ˆâ–      | 16917/50000 [00:06<00:04, 7661.68 examples/s]\n",
      "Generating unsupervised split:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 17694/50000 [00:06<00:04, 7691.90 examples/s]\n",
      "Generating unsupervised split:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 18478/50000 [00:06<00:04, 7735.78 examples/s]\n",
      "Generating unsupervised split:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 19255/50000 [00:06<00:03, 7731.02 examples/s]\n",
      "Generating unsupervised split:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20030/50000 [00:06<00:04, 7148.68 examples/s]\n",
      "Generating unsupervised split:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20793/50000 [00:06<00:04, 7283.75 examples/s]\n",
      "Generating unsupervised split:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 21569/50000 [00:07<00:03, 7420.43 examples/s]\n",
      "Generating unsupervised split:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22342/50000 [00:07<00:03, 7507.59 examples/s]\n",
      "Generating unsupervised split:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23122/50000 [00:07<00:03, 7592.48 examples/s]\n",
      "Generating unsupervised split:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23899/50000 [00:07<00:03, 7644.03 examples/s]\n",
      "Generating unsupervised split:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 24672/50000 [00:07<00:03, 7668.08 examples/s]\n",
      "Generating unsupervised split:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25450/50000 [00:07<00:03, 7698.88 examples/s]\n",
      "Generating unsupervised split:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26235/50000 [00:07<00:03, 7742.71 examples/s]\n",
      "Generating unsupervised split:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27018/50000 [00:07<00:02, 7767.33 examples/s]\n",
      "Generating unsupervised split:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27808/50000 [00:07<00:02, 7806.71 examples/s]\n",
      "Generating unsupervised split:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 28590/50000 [00:07<00:02, 7798.17 examples/s]\n",
      "Generating unsupervised split:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29371/50000 [00:08<00:02, 7779.65 examples/s]\n",
      "Generating unsupervised split:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30150/50000 [00:08<00:02, 6941.70 examples/s]\n",
      "Generating unsupervised split:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30917/50000 [00:08<00:02, 7140.39 examples/s]\n",
      "Generating unsupervised split:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32466/50000 [00:08<00:02, 7437.26 examples/s]\n",
      "Generating unsupervised split:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 33246/50000 [00:08<00:02, 7542.43 examples/s]\n",
      "Generating unsupervised split:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34015/50000 [00:08<00:02, 7585.70 examples/s]\n",
      "Generating unsupervised split:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 34801/50000 [00:08<00:01, 7666.60 examples/s]\n",
      "Generating unsupervised split:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35584/50000 [00:08<00:01, 7708.43 examples/s]\n",
      "Generating unsupervised split:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 36366/50000 [00:08<00:01, 7739.46 examples/s]\n",
      "Generating unsupervised split:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37150/50000 [00:09<00:01, 7767.20 examples/s]\n",
      "Generating unsupervised split:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37932/50000 [00:09<00:01, 7781.47 examples/s]\n",
      "Generating unsupervised split:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 38714/50000 [00:09<00:01, 7792.02 examples/s]\n",
      "Generating unsupervised split:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39494/50000 [00:09<00:01, 7767.25 examples/s]\n",
      "Generating unsupervised split:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40272/50000 [00:09<00:01, 7013.04 examples/s]\n",
      "Generating unsupervised split:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40997/50000 [00:09<00:01, 7076.94 examples/s]\n",
      "Generating unsupervised split:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 41771/50000 [00:09<00:01, 7264.47 examples/s]\n",
      "Generating unsupervised split:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 42547/50000 [00:09<00:01, 7405.72 examples/s]\n",
      "Generating unsupervised split:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 43326/50000 [00:09<00:00, 7517.15 examples/s]\n",
      "Generating unsupervised split:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44106/50000 [00:09<00:00, 7598.74 examples/s]\n",
      "Generating unsupervised split:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 44887/50000 [00:10<00:00, 7660.37 examples/s]\n",
      "Generating unsupervised split:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45667/50000 [00:10<00:00, 7699.33 examples/s]\n",
      "Generating unsupervised split:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 46449/50000 [00:10<00:00, 7732.82 examples/s]\n",
      "Generating unsupervised split:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47224/50000 [00:10<00:00, 7734.67 examples/s]\n",
      "Generating unsupervised split:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47999/50000 [00:10<00:00, 7691.00 examples/s]\n",
      "Generating unsupervised split:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 48783/50000 [00:10<00:00, 7733.08 examples/s]\n",
      "Generating unsupervised split:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 49566/50000 [00:10<00:00, 7758.67 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 710.50it/s]                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Dataset imdb downloaded and prepared to /home/ray/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 32.8kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00<00:00, 559kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 4.77MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 7.86MB/s]\n",
      "  0%|          | 0/25 [00:00<?, ?ba/s]\n",
      "  4%|â–         | 1/25 [00:00<00:15,  1.52ba/s]\n",
      "  8%|â–Š         | 2/25 [00:01<00:14,  1.61ba/s]\n",
      " 12%|â–ˆâ–        | 3/25 [00:01<00:13,  1.60ba/s]\n",
      " 16%|â–ˆâ–Œ        | 4/25 [00:02<00:13,  1.61ba/s]\n",
      " 20%|â–ˆâ–ˆ        | 5/25 [00:03<00:13,  1.53ba/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 6/25 [00:03<00:12,  1.54ba/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:04<00:11,  1.56ba/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:05<00:10,  1.55ba/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:05<00:10,  1.55ba/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:06<00:09,  1.58ba/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:06<00:08,  1.61ba/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:07<00:08,  1.62ba/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:08<00:07,  1.62ba/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:08<00:06,  1.59ba/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:09<00:06,  1.59ba/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:10<00:05,  1.58ba/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:10<00:05,  1.55ba/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:11<00:04,  1.58ba/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:12<00:03,  1.57ba/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:12<00:03,  1.56ba/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:13<00:02,  1.53ba/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:14<00:01,  1.53ba/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:14<00:01,  1.56ba/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:15<00:00,  1.57ba/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:15<00:00,  1.51ba/s]\n",
      "  0%|          | 0/25 [00:00<?, ?ba/s]\n",
      "  4%|â–         | 1/25 [00:00<00:14,  1.67ba/s]\n",
      "  8%|â–Š         | 2/25 [00:01<00:13,  1.68ba/s]\n",
      " 12%|â–ˆâ–        | 3/25 [00:01<00:13,  1.65ba/s]\n",
      " 16%|â–ˆâ–Œ        | 4/25 [00:02<00:12,  1.66ba/s]\n",
      " 20%|â–ˆâ–ˆ        | 5/25 [00:03<00:12,  1.64ba/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 6/25 [00:03<00:11,  1.61ba/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:04<00:11,  1.63ba/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:04<00:10,  1.61ba/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:05<00:10,  1.58ba/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:06<00:09,  1.56ba/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:06<00:08,  1.60ba/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:07<00:08,  1.60ba/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:08<00:07,  1.56ba/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:08<00:06,  1.59ba/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:09<00:06,  1.60ba/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:09<00:05,  1.59ba/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:10<00:05,  1.58ba/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:11<00:04,  1.58ba/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:11<00:03,  1.58ba/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:12<00:03,  1.61ba/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:13<00:02,  1.63ba/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:13<00:01,  1.60ba/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:14<00:01,  1.59ba/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:14<00:00,  1.60ba/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:15<00:00,  1.54ba/s]\n",
      "  0%|          | 0/50 [00:00<?, ?ba/s]\n",
      "  2%|â–         | 1/50 [00:00<00:29,  1.66ba/s]\n",
      "  4%|â–         | 2/50 [00:01<00:29,  1.63ba/s]\n",
      "  6%|â–Œ         | 3/50 [00:01<00:30,  1.57ba/s]\n",
      "  8%|â–Š         | 4/50 [00:02<00:30,  1.52ba/s]\n",
      " 10%|â–ˆ         | 5/50 [00:03<00:29,  1.52ba/s]\n",
      " 12%|â–ˆâ–        | 6/50 [00:03<00:28,  1.56ba/s]\n",
      " 14%|â–ˆâ–        | 7/50 [00:04<00:27,  1.56ba/s]\n",
      " 16%|â–ˆâ–Œ        | 8/50 [00:05<00:27,  1.55ba/s]\n",
      " 18%|â–ˆâ–Š        | 9/50 [00:05<00:25,  1.59ba/s]\n",
      " 20%|â–ˆâ–ˆ        | 10/50 [00:06<00:25,  1.57ba/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 11/50 [00:07<00:24,  1.57ba/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 12/50 [00:07<00:23,  1.58ba/s]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:08<00:23,  1.60ba/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:08<00:22,  1.58ba/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:09<00:22,  1.55ba/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:10<00:21,  1.55ba/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:10<00:21,  1.54ba/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:11<00:20,  1.55ba/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:12<00:20,  1.55ba/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:12<00:19,  1.53ba/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:13<00:19,  1.51ba/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:14<00:18,  1.54ba/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:14<00:17,  1.55ba/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:15<00:16,  1.57ba/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:16<00:15,  1.57ba/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:16<00:15,  1.58ba/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:17<00:14,  1.57ba/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:17<00:13,  1.61ba/s]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:18<00:13,  1.58ba/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:19<00:12,  1.59ba/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:19<00:12,  1.55ba/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:20<00:11,  1.55ba/s]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:21<00:10,  1.56ba/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:21<00:10,  1.59ba/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:22<00:09,  1.61ba/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:22<00:08,  1.60ba/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:23<00:08,  1.55ba/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:24<00:07,  1.58ba/s]\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [00:24<00:06,  1.58ba/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [00:25<00:06,  1.60ba/s]\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [00:26<00:05,  1.56ba/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [00:26<00:05,  1.57ba/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [00:27<00:04,  1.58ba/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [00:28<00:03,  1.58ba/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [00:28<00:03,  1.58ba/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [00:29<00:02,  1.57ba/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [00:29<00:01,  1.56ba/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [00:30<00:01,  1.55ba/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [00:31<00:00,  1.56ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m len of train Dataset({\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m     num_rows: 100\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m }) and test Dataset({\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m     num_rows: 100\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m })\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [00:31<00:00,  1.53ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:03 (running for 00:00:05.19)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 6.9/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m 2022-11-04 04:25:06,390\tINFO torch.py:346 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m 2022-11-04 04:25:06,389\tINFO torch.py:346 -- Setting up process group for: env:// [rank=2, world_size=4]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m 2022-11-04 04:25:06,391\tINFO torch.py:346 -- Setting up process group for: env:// [rank=3, world_size=4]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m 2022-11-04 04:25:06,393\tINFO torch.py:346 -- Setting up process group for: env:// [rank=1, world_size=4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:08 (running for 00:00:10.19)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 7.7/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00<00:00, 665kB/s]\n",
      "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s] \n",
      "Downloading:   2%|â–         | 5.95M/268M [00:00<00:04, 59.5MB/s]\n",
      "Downloading:   5%|â–         | 12.5M/268M [00:00<00:04, 62.8MB/s]\n",
      "Downloading:   7%|â–‹         | 19.0M/268M [00:00<00:03, 63.9MB/s]\n",
      "Downloading:  10%|â–‰         | 25.6M/268M [00:00<00:03, 64.6MB/s]\n",
      "Downloading:  12%|â–ˆâ–        | 32.1M/268M [00:00<00:03, 64.9MB/s]\n",
      "Downloading:  14%|â–ˆâ–        | 38.6M/268M [00:00<00:03, 65.0MB/s]\n",
      "Downloading:  17%|â–ˆâ–‹        | 45.2M/268M [00:00<00:03, 65.1MB/s]\n",
      "Downloading:  19%|â–ˆâ–‰        | 51.7M/268M [00:00<00:03, 65.3MB/s]\n",
      "Downloading:  22%|â–ˆâ–ˆâ–       | 58.3M/268M [00:00<00:03, 65.5MB/s]\n",
      "Downloading:  24%|â–ˆâ–ˆâ–       | 64.9M/268M [00:01<00:03, 65.4MB/s]\n",
      "Downloading:  27%|â–ˆâ–ˆâ–‹       | 71.4M/268M [00:01<00:03, 65.4MB/s]\n",
      "Downloading:  29%|â–ˆâ–ˆâ–‰       | 78.0M/268M [00:01<00:02, 65.5MB/s]\n",
      "Downloading:  32%|â–ˆâ–ˆâ–ˆâ–      | 84.5M/268M [00:01<00:02, 65.4MB/s]\n",
      "Downloading:  34%|â–ˆâ–ˆâ–ˆâ–      | 91.1M/268M [00:01<00:02, 65.4MB/s]\n",
      "Downloading:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 97.6M/268M [00:01<00:02, 65.4MB/s]\n",
      "Downloading:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 104M/268M [00:01<00:02, 65.4MB/s] \n",
      "Downloading:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 111M/268M [00:01<00:02, 65.4MB/s]\n",
      "Downloading:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 117M/268M [00:01<00:02, 65.5MB/s]\n",
      "Downloading:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 124M/268M [00:01<00:02, 65.8MB/s]\n",
      "Downloading:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131M/268M [00:02<00:02, 65.7MB/s]\n",
      "Downloading:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137M/268M [00:02<00:01, 65.6MB/s]\n",
      "Downloading:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144M/268M [00:02<00:01, 65.8MB/s]\n",
      "Downloading:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150M/268M [00:02<00:01, 65.8MB/s]\n",
      "Downloading:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157M/268M [00:02<00:01, 65.8MB/s]\n",
      "Downloading:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 163M/268M [00:02<00:01, 65.6MB/s]\n",
      "Downloading:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 170M/268M [00:02<00:01, 65.5MB/s]\n",
      "Downloading:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 177M/268M [00:02<00:01, 65.6MB/s]\n",
      "Downloading:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 183M/268M [00:02<00:01, 65.5MB/s]\n",
      "Downloading:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 190M/268M [00:02<00:01, 65.5MB/s]\n",
      "Downloading:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 196M/268M [00:03<00:01, 65.5MB/s]\n",
      "Downloading:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 203M/268M [00:03<00:00, 65.4MB/s]\n",
      "Downloading:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 209M/268M [00:03<00:00, 65.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:13 (running for 00:00:15.19)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 8.0/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 216M/268M [00:03<00:00, 65.4MB/s]\n",
      "Downloading:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 222M/268M [00:03<00:00, 65.3MB/s]\n",
      "Downloading:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 229M/268M [00:03<00:00, 65.6MB/s]\n",
      "Downloading:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 236M/268M [00:03<00:00, 65.7MB/s]\n",
      "Downloading:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 242M/268M [00:03<00:00, 65.8MB/s]\n",
      "Downloading:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 249M/268M [00:03<00:00, 65.9MB/s]\n",
      "Downloading:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 255M/268M [00:03<00:00, 65.8MB/s]\n",
      "Downloading:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 262M/268M [00:04<00:00, 65.7MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:04<00:00, 65.4MB/s]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m /tmp/ray/session_2022-11-03_21-16-35_318329_7/runtime_resources/pip/4a36d7bd0bbff8fccea52f9c0d942dd63707933f/virtualenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m /tmp/ray/session_2022-11-03_21-16-35_318329_7/runtime_resources/pip/4a36d7bd0bbff8fccea52f9c0d942dd63707933f/virtualenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m /tmp/ray/session_2022-11-03_21-16-35_318329_7/runtime_resources/pip/4a36d7bd0bbff8fccea52f9c0d942dd63707933f/virtualenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m /tmp/ray/session_2022-11-03_21-16-35_318329_7/runtime_resources/pip/4a36d7bd0bbff8fccea52f9c0d942dd63707933f/virtualenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m ***** Running training *****\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   Num examples = 6250\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   Num Epochs = 1\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   Instantaneous batch size per device = 16\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   Total optimization steps = 391\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m   Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:18 (running for 00:00:20.20)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 13.0/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=184, ip=10.128.64.16)\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=185, ip=10.128.64.16)\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=183, ip=10.128.64.16)\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:23 (running for 00:00:25.20)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:28 (running for 00:00:30.20)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:33 (running for 00:00:35.21)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:38 (running for 00:00:40.21)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:43 (running for 00:00:45.21)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:48 (running for 00:00:50.22)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:53 (running for 00:00:55.22)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:25:58 (running for 00:01:00.22)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:03 (running for 00:01:05.23)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:08 (running for 00:01:10.23)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:13 (running for 00:01:15.23)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:18 (running for 00:01:20.24)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:23 (running for 00:01:25.24)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:28 (running for 00:01:30.24)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:33 (running for 00:01:35.25)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:38 (running for 00:01:40.25)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:43 (running for 00:01:45.25)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:48 (running for 00:01:50.25)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:53 (running for 00:01:55.26)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:26:58 (running for 00:02:00.26)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:27:03 (running for 00:02:05.27)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 14.1/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m Saving model checkpoint to /tmp/hf_imdb/test/checkpoint-391\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m Configuration saved in /tmp/hf_imdb/test/checkpoint-391/config.json\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m Model weights saved in /tmp/hf_imdb/test/checkpoint-391/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m {'train_runtime': 109.3646, 'train_samples_per_second': 57.148, 'train_steps_per_second': 3.575, 'train_loss': 0.2757800363213815, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result for HuggingFaceTrainer_50527_00000:\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   _time_this_iter_s: 117.43207788467407\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   _timestamp: 1667561227\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   _training_iteration: 1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   date: 2022-11-04_04-27-07\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   epoch: 1.0\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   experiment_id: 5c44a90f5c474864aee374a2ac1905e1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   hostname: hfgputest-worker-small-group-hfgputest-8f4mg\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   iterations_since_restore: 1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   node_ip: 10.128.64.16\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   pid: 146\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   should_checkpoint: true\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   step: 391\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   time_since_restore: 124.04675316810608\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   time_this_iter_s: 124.04675316810608\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   time_total_s: 124.04675316810608\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   timestamp: 1667561227\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_loss: 0.2757800363213815\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_runtime: 109.3646\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_samples_per_second: 57.148\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_steps_per_second: 3.575\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   training_iteration: 1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   trial_id: '50527_00000'\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   warmup_time: 0.004158496856689453\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=182, ip=10.128.64.16)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:27:16 (running for 00:02:18.35)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 15.8/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 5.0/10 CPUs, 4.0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+--------+------------------+-----------------+----------------------------+--------------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status   | loc              |   iter |   total time (s) |   train_runtime |   train_samples_per_second |   train_steps_per_second |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+----------+------------------+--------+------------------+-----------------+----------------------------+--------------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | RUNNING  | 10.128.64.16:146 |      1 |          124.047 |         109.365 |                     57.148 |                    3.575 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+----------+------------------+--------+------------------+-----------------+----------------------------+--------------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m 2022-11-04 04:27:16,323\tWARNING util.py:214 -- The `process_trial_save` operation took 9.089 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m 2022-11-04 04:27:16,323\tWARNING trial_runner.py:856 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result for HuggingFaceTrainer_50527_00000:\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   _time_this_iter_s: 117.43207788467407\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   _timestamp: 1667561227\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   _training_iteration: 1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   date: 2022-11-04_04-27-07\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   done: true\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   epoch: 1.0\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   experiment_id: 5c44a90f5c474864aee374a2ac1905e1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   experiment_tag: '0'\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   hostname: hfgputest-worker-small-group-hfgputest-8f4mg\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   iterations_since_restore: 1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   node_ip: 10.128.64.16\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   pid: 146\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   should_checkpoint: true\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   step: 391\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   time_since_restore: 124.04675316810608\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   time_this_iter_s: 124.04675316810608\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   time_total_s: 124.04675316810608\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   timestamp: 1667561227\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_loss: 0.2757800363213815\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_runtime: 109.3646\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_samples_per_second: 57.148\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   train_steps_per_second: 3.575\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   training_iteration: 1\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   trial_id: '50527_00000'\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   warmup_time: 0.004158496856689453\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Current time: 2022-11-04 04:27:19 (running for 00:02:21.95)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Memory usage on this node: 8.8/240.1 GiB\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Resources requested: 0/10 CPUs, 0/4 GPUs, 0.0/22.35 GiB heap, 0.0/6.59 GiB objects\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Result logdir: /home/ray/ray_results/HuggingFaceTrainer_2022-11-04_04-24-57\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m Number of trials: 1/1 (1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+------------+------------------+--------+------------------+-----------------+----------------------------+--------------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | Trial name                     | status     | loc              |   iter |   total time (s) |   train_runtime |   train_samples_per_second |   train_steps_per_second |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m |--------------------------------+------------+------------------+--------+------------------+-----------------+----------------------------+--------------------------|\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m | HuggingFaceTrainer_50527_00000 | TERMINATED | 10.128.64.16:146 |      1 |          124.047 |         109.365 |                     57.148 |                    3.575 |\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m +--------------------------------+------------+------------------+--------+------------------+-----------------+----------------------------+--------------------------+\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m 2022-11-04 04:27:19,924\tWARNING util.py:214 -- The `process_trial_save` operation took 2.028 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(train_fn pid=249)\u001b[0m 2022-11-04 04:27:20,036\tINFO tune.py:747 -- Total run time: 142.25 seconds (141.95 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "#call the above cell as a remote ray function\n",
    "ray.get(train_fn.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39e6a57f-d046-4919-b7bf-710e6c5f34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec421113-0e49-4043-a3b5-66efa5021cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a183b-5e8e-4adb-b9a6-a349e13512a0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As shown in the above example, you can easily run your Huggingface transfer learning tasks easily and natively on CodeFlare. You can scale them from 1 to n GPUs without requiring you to make any significant code changes and leveraging the native Huggingface trainer. \n",
    "\n",
    "Also refer to additional notebooks that showcase other use cases\n",
    "In our next notebook [./02_codeflare_workflows_encoding.ipynb ] shows an sklearn example and how you can leverage workflows to run experiment pipelines and explore multiple pipelines in parallel on CodeFlare cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677c868-a052-4893-9493-6f1dacd8fa27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
